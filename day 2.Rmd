---
title: "day 2"
author: "Brenna Kelly"
date: "2023-07-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Estimating IP weights via modeling

Generally people assume that the more you account for, the more you can trust the conditional ignorability assumption. If we perfectly predict who is in tx/ctrl, this is a problem — we have a positivity violation. In other words, you don't want propensity scores to be 0 or 1. Even close to 0 or 1, the weights get quite large. 
* Anywhere in the probability space, there is a probability of being exposed or not — if there is a space when you are with certainty goingt o be treated, you can't draw causal inference in that probability space.

```{r}
nhefs0 <- read.csv("data/nhefs.csv")
names(nhefs0)

fit <- glm(qsmk ~ as.factor(sex) + as.factor(race) + age + I(age^2) +
  as.factor(education) + smokeintensity +
  I(smokeintensity^2) + smokeyrs + I(smokeyrs^2) +
  as.factor(exercise) + as.factor(active) + wt71 + I(wt71^2),
           family = binomial(), data = nhefs0)

summary(fit)

p.qsmk.obs <- ifelse(nhefs0$qsmk == 0, 
                     1 - predict(fit, type = "response"), # 1 - propensity score; to est propensity score for someone
                     predict(fit, type = "response")) # otherwise, estimated propensity score
nhefs0$w <- 1/p.qsmk.obs

# always check weights; if some are quite large, the next step could be massively biased.
summary(nhefs0$w) # checking distribution of weights
summary(p.qsmk.obs)
hist(p.qsmk.obs)

# simple regression > no covariates; but with weights based on propensity score
glm.obj <- glm(wt82_71~qsmk, data = nhefs0, weights = w)

summary(glm.obj)

# what are the two counterfactual means?
glm.obj$coefficients[1] # Y0
glm.obj$coefficients[1] + glm.obj$coefficients[2] # Y1

summary(glm(wt82_71~qsmk, data = subset(nhefs0, cens == 0), weights = w))
summary(glm(wt82_71~qsmk, data = subset(nhefs0, cens == 1), weights = w))

```
Assumes we've correctly specified the y/x. Machine learning (esp. ensemble approaches) might 
Double machine learning approaches use ML for both stages.
